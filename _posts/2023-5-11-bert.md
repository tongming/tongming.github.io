bert的基础知识
第一节：Bert中的Embeddings
Bert输入Embeddings包含三个部分，第一部分为word的embeddings，第二部分为position的embeddings，第三部分为token_type的embeddings，具体的代码如下（为了不使得代码冗长，删除了部分不重要的代码）：
class TFBertEmbeddings(tf.keras.layers.Layer):
    """Construct the embeddings from word, position and token_type embeddings."""

    def __init__(self, config: BertConfig, **kwargs):
        super().__init__(**kwargs)

        self.config = config
        self.hidden_size = config.hidden_size
        self.max_position_embeddings = config.max_position_embeddings
        self.initializer_range = config.initializer_range
        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm")
        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)

    def build(self, input_shape: tf.TensorShape):
        with tf.name_scope("word_embeddings"):
            self.weight = self.add_weight(
                name="weight",
                shape=[self.config.vocab_size, self.hidden_size],
                initializer=get_initializer(self.initializer_range),
            )

        with tf.name_scope("token_type_embeddings"):
            self.token_type_embeddings = self.add_weight(
                name="embeddings",
                shape=[self.config.type_vocab_size, self.hidden_size],
                initializer=get_initializer(self.initializer_range),
            )

        with tf.name_scope("position_embeddings"):
            self.position_embeddings = self.add_weight(
                name="embeddings",
                shape=[self.max_position_embeddings, self.hidden_size],
                initializer=get_initializer(self.initializer_range),
            )

        super().build(input_shape)

    def call(
        self,
        input_ids: tf.Tensor = None,
        position_ids: tf.Tensor = None,
        token_type_ids: tf.Tensor = None,
        inputs_embeds: tf.Tensor = None,
        past_key_values_length=0,
        training: bool = False,
    ) -> tf.Tensor:
        """
        Applies embedding based on inputs tensor.

        Returns:
            final_embeddings (`tf.Tensor`): output embedding tensor.
        """
        if input_ids is None and inputs_embeds is None:
            raise ValueError("Need to provide either `input_ids` or `input_embeds`.")

        input_shape = shape_list(inputs_embeds)[:-1]

        if token_type_ids is None:
            token_type_ids = tf.fill(dims=input_shape, value=0)

        if position_ids is None:
            position_ids = tf.expand_dims(
                tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0
            )

        position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)
        token_type_embeds = tf.gather(params=self.token_type_embeddings, indices=token_type_ids)
        final_embeddings = inputs_embeds + position_embeds + token_type_embeds
        final_embeddings = self.LayerNorm(inputs=final_embeddings)
        final_embeddings = self.dropout(inputs=final_embeddings, training=training)

        return final_embeddings
最终输出的embeddings为三个embeddings相加，同时加上了LayerNorm和dropout，这段代码有两个知识点：
LayerNorm：LayerNorm的作用是对每个样本embedding的每个维度进行归一化，使得每个特征的均值为0，方差为1
dropout：dropout的作用是在训练的时候随机的将一些神经元置为0，使得模型不会过拟合，dropout在training和inference时的行为是不一样的，training时会随机的将一些神经元置为0，inference时不会做任何操作，这里的training参数就是用来控制这个行为的
以bert-base-chinese为例，包含的vocab字典大小为21128，token_type为2，max_position为512，embedding的长度为768，Embedding层的参数量为(21128+2+512)*768=16267776，这里的参数量是指Embedding层的参数量，不包含LayerNorm和dropout的参数量，LayerNorm的参数量为768*2=1536，dropout的参数量为0，所以Embedding层的总参数量为16269312
第二节：Bert中Embeddings的input_ids，position_ids，token_type_ids怎么生成的


