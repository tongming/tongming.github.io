---
layout: post
title: "Tranformers系列"
subtitle:   "Transformers系列之Bert详解"
date:       2023-05-17 12:00:00
author:     "Ming Tong"
catalog: true
tags:
    - LLM
---
##bert的基础知识
###Bert模型简要介绍
Bert是一个基于深度transformer编码器结构的模型。利用预训练+微调的方式去解决其他的任务，区别于之前多数基于于特征提取的方法（如Elmo、Word2vec）。预训练任务包含两个目标，分别为掩码语言模型以及预测下一个句子的目标。
在下面的文章中，我们会从模型的基础结构和预训练任务开始，逐步深入到Bert的细节，最后再介绍一些Bert的应用。
###Bert中的Embeddings
####Bert中的Embeddings包含哪些部分
Bert输入Embeddings包含三个部分，第一部分为word的embeddings，第二部分为position的embeddings，第三部分为token_type的embeddings，具体的代码如下（为了不使得代码冗长，删除了部分不重要的代码）
 ```python
class TFBertEmbeddings(tf.keras.layers.Layer):
    """从token、token类型以及位置构建embeddings向量"""

    def __init__(self, config: BertConfig, **kwargs):
        super().__init__(**kwargs)

        self.config = config
        self.hidden_size = config.hidden_size
        self.max_position_embeddings = config.max_position_embeddings
        self.initializer_range = config.initializer_range
        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name="LayerNorm")
        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)

    def build(self, input_shape: tf.TensorShape):
        with tf.name_scope("word_embeddings"):
            self.weight = self.add_weight(
                name="weight",
                shape=[self.config.vocab_size, self.hidden_size],
                initializer=get_initializer(self.initializer_range),
            )

        with tf.name_scope("token_type_embeddings"):
            self.token_type_embeddings = self.add_weight(
                name="embeddings",
                shape=[self.config.type_vocab_size, self.hidden_size],
                initializer=get_initializer(self.initializer_range),
            )

        with tf.name_scope("position_embeddings"):
            self.position_embeddings = self.add_weight(
                name="embeddings",
                shape=[self.max_position_embeddings, self.hidden_size],
                initializer=get_initializer(self.initializer_range),
            )

        super().build(input_shape)

    def call(
        self,
        input_ids: tf.Tensor = None,
        position_ids: tf.Tensor = None,
        token_type_ids: tf.Tensor = None,
        inputs_embeds: tf.Tensor = None,
        past_key_values_length=0,
        training: bool = False,
    ) -> tf.Tensor:
        if input_ids is None and inputs_embeds is None:
            raise ValueError("Need to provide either `input_ids` or `input_embeds`.")

        input_shape = shape_list(inputs_embeds)[:-1]

        if token_type_ids is None:
            token_type_ids = tf.fill(dims=input_shape, value=0)

        if position_ids is None:
            position_ids = tf.expand_dims(
                tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0
            )

        position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)
        token_type_embeds = tf.gather(params=self.token_type_embeddings, indices=token_type_ids)
        final_embeddings = inputs_embeds + position_embeds + token_type_embeds
        final_embeddings = self.LayerNorm(inputs=final_embeddings)
        final_embeddings = self.dropout(inputs=final_embeddings, training=training)
        return final_embeddings
```     
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="/img/emb_plus.png">
    <br>
    <div style="
    display: inline-block;
    padding: 2px;">embedding相加示意图，图片来源standford cs224n</div>
</center>
最终输出的embeddings为三个embeddings相加，同时加上了LayerNorm和dropout，这段代码的模型结构有两个知识点：
```
LayerNorm：LayerNorm的作用是对每个样本embedding的每个维度进行归一化，使得每个特征的均值为0，方差为1
dropout：dropout的作用是在训练的时候随机的将一些神经元置为0，使得模型不会过拟合，dropout在training和inference时的行为是不一样的，training时会随机的将一些神经元置为0，inference时不会做任何操作，这里的training参数就是用来控制这个行为的
```
position_ids和token_type_ids在模型中起到什么作用了？position_ids是用来表示每个token的位置的，token_type_ids是用来表示每个token的类型的，其中position_ids在Attention is All you need的原始论文中引入，主要解决自注意力机制不能够区分位置信息的问题，token_type_ids是Bert中引入的，主要解决Bert中的句子对任务的问题，用来区别是第一个句子还是第二个句子。
以bert-base-chinese为例，包含的vocab字典大小为21128，token_type为2，max_position为512，embedding的长度为768，Embedding层的参数量为(21128+2+512)\*768=16267776，这里的参数量是指Embedding层的参数量，不包含LayerNorm和dropout的参数量，LayerNorm的参数量为768*2=1536，dropout的参数量为0，所以Embedding层的总参数量为16269312
####Bert中Embeddings的input_ids，position_ids，token_type_ids怎么生成的
####WordPiece tokenization算法
WordPiece tokenization算法是一种基于字典的分词算法，是Google为Bert开发的一种分词方法，后来的DistilBert，MobileBert等模型也是基于WordPiece的分词方法。它的基本思想是将一个词分成多个字，然后根据字典来进行分词，这里的字典是指WordPiece的字典，WordPiece的字典是通过训练语料来得到的，具体的算法可以参见Huggingface中WordPiece tokenization的教程，这里做个简单的介绍：
1、把word按照字母切分，加上前缀（bert是\#\#的前缀)，如 word  ==> w \#\#o \#\#r \#\#d
2、合并阶段，合并的时候，不同于BPE，合并准则为：score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)，按照分数高低合并，直到达到设定的词典大小
利用字典分词的时候，利用最大匹配的方式去分词，如果不在词典中，则返回'[UNK]'特殊字符，具体的代码如下：
 ```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```
假设vocab为：['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '\#\#b', '\#\#c', '\#\#d', '\#\#e', '\#\#f', '\#\#g', '\#\#h', '\#\#i', '\#\#k','\#\#l', '\#\#m', '\#\#n','\#\#o', '\#\#p', '\#\#r', '\#\#s', '\#\#t', '\#\#u', '\#\#v', '\#\#w', '\#\#y', '\#\#z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '\#\#fu', 'Fa', 'Fac', '\#\#ct', '\#\#ful', '\#\#full', '\#\#fully', 'Th', 'ch', '\#\#hm', 'cha', 'chap', 'chapt', '\#\#thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '\#\#thms','\#\#za', '\#\#zat', '\#\#ut']
那么encode_word("Hugging") => ['Hugg', '\#\#i', '\#\#n', '\#\#g']；encode_word("HOgging") => ['[UNK]']
embedding的生成如下图所示
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="/img/subword_toking.png">
    <br>
    <div style="
    display: inline-block;
    padding: 2px;">分词示意图，图片来源standford cs224n</div>
</center>
当前汉语Bert预训练模型一般直接用的是汉字，在利用WordPiece的时候，会事先在每个汉字周边都插入空格，对每个字分成一个词，因此最终的效果等价于每个汉字会是一个词。
####利用Transformer中的Tokenizer实现对中文的分词处理
AutoTokenizer是Huggingface Transformer中的一个类，可以根据模型的名称自动的选择对应的Tokenizer，这里以bert-base-chinese为例，代码如下：
tokenizer = AutoTokenizer.from_pretrained('model_dir/bert_base_chinese')
这里我们假设下载好的模型存在model_dir/bert_base_chinese的文件夹中，AutoTokenizer会自动的选择BertTokenizer，然后加载对应的词典，利用相应的词典就可以对语料进行编码。
为了用实际的数据集例子，我们介绍一下golden-horse数据集，这个数据集是一个中文命名实体识别的数据集，可以在github搜索下载。数据集的处理代码如下：
 ```python
import pandas as pd
train_file = 'data/weiboNER_2nd_conll.train'
dev_file = 'data/weiboNER_2nd_conll.dev'
test_file = 'data/weiboNER_2nd_conll.test'

sentence_id = 0
def encode(s):
    global sentence_id
    if pd.isna(s):
        sentence_id += 1
        return None
    else:
        return str(sentence_id)
def get_dataset(filename, output_label=False):
    data = pd.read_csv(filename, sep='\t', header=None, names=['token', 'POS'], skip_blank_lines=False,encoding='utf-8')
    data['word'] = data['token'].fillna('').map(lambda i: i[:-1])
    
    data['sentence'] = data['token'].apply(encode)
    data.dropna(how="any", inplace=True)
    if output_label:
        label2id = {k: v for v, k in enumerate(data.POS.unique())}
        id2label = {v: k for v, k in enumerate(data.POS.unique())}
        
    # let's create a new column called "sentence" which groups the words by sentence 
    data['Sentence'] = data[['sentence','word','POS']].groupby(['sentence'])['word'].transform(lambda x: ''.join(x))
    # let's also create a new column called "word_labels" which groups the tags by sentence 
    data['word_labels'] = data[['sentence','word','POS']].groupby(['sentence'])['POS'].transform(lambda x: ','.join(x))
    data = data[["Sentence", "word_labels"]].drop_duplicates().reset_index(drop=True)
    if output_label:
        return data, label2id, id2label
    else:
        return data
train_dataset, label2id, id2label = get_dataset(train_file, True)
test_dataset = get_dataset(test_file)
dev_dataset = get_dataset(dev_file)
```
处理后的数据格式如下：
![avatar](/img/golden_horse_data.jpg)
为了得到编码后的token_id，我们只需要一句代码就可以实现:
```python
encodings = tokenizer(sentence,padding=True, truncation=True,max_length=30, return_attention_mask=True,return_offsets_mapping=True)

上述代码确定了最大长度为30，如果句子长度不足30，则会自动补充[PAD]，如果句子长度超过30，则会自动截断（实际上会自动上述代码会自动在每个句子插入一个特殊的开始字符[CLS]和结束字符[SEP]，因此最大的句子长度为28）。返回的encodings是一个字典，包含了input_ids, attention_mask, token_type_ids, offset_mapping四个参数，其中input_ids就是我们需要的编码后的token_id。attention_mask的作用是告诉模型哪些是真实的输入，哪些是[PAD]，这样模型就不会把[PAD]的部分也当做输入进行计算。
offset_mapping是一个数组，数组的元素一个二元的元组。这个元组的两个元素分别表示token_id的起始位置和结束位置，主要用来对应原始的词以及其标签和编码后的token_id，因为可能一个单词在编码后会对应多个token_id，这个时候，之前的标签就和token_id不能一一对应，需要我们利用offset_mapping去修复。具体的修复代码如下：
```python
import numpy as np
def encode_tags(tags, encodings):
    labels = [[label2id[tag] for tag in doc.split(',')] for doc in tags]
    encoded_labels = []
    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):
        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100  # label -100模型不会训练
        for i, (beg, end) in enumerate(doc_offset):
            if end != 0:
                doc_enc_labels[i] = doc_labels[beg]
        encoded_labels.append(doc_enc_labels.tolist())
    return encoded_labels
encoded_labels = encode_tags(labels, encodings)
```

###Attention结构
Attention结构有各种各种，应用比较广泛的是Transformer论文中提出的基于缩放的内积注意力机制，其公式如下：
attn(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V





